{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据处理，根据当前场景语料生成腾讯词向量和vocab_words.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T08:24:23.421168Z",
     "start_time": "2021-06-03T08:24:23.418935Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T06:17:05.666736Z",
     "start_time": "2021-06-03T05:52:28.220176Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "file = '~/pretrained/tencent_embedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(file, binary=False) # 加载时间比较长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 根据场景语料build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T07:01:14.343671Z",
     "start_time": "2021-06-03T07:01:12.626246Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/p5/knwww3f575n5ykrj4r0706d40000gn/T/jieba.cache\n",
      "Loading model cost 0.738 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import jieba\n",
    "jieba.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.txt文件格式\n",
    "'''\n",
    "体验2D巅峰 倚天屠龙记十大创新概览\t8\n",
    "60年铁树开花形状似玉米芯(组图)\t5\n",
    "同步A股首秀：港股缩量回调\t2\n",
    "中青宝sg现场抓拍 兔子舞热辣表演\t8\n",
    "锌价难续去年辉煌\t0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T07:01:14.349690Z",
     "start_time": "2021-06-03T07:01:14.346384Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = jieba.lcut\n",
    "train_path = ['./THUCNews/data/train.txt',\n",
    "             './THUCNews/data/dev.txt',\n",
    "             './THUCNews/data/test.txt']\n",
    "vocab_dir = \"./THUCNews/data/vocab_words.pkl\"\n",
    "UNK, PAD = '<UNK>', '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T08:49:21.495175Z",
     "start_time": "2021-06-03T08:49:21.488881Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(files_path, tokenizer, max_size, min_freq):\n",
    "    #max_size取None则表示不指定max size大小\n",
    "    vocab_dic = {}\n",
    "    if isinstance(files_path, str):\n",
    "        raise ValueError('please input paths in list format!')\n",
    "    for file_path in files_path:\n",
    "        with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                content = lin.split('\\t')[0]\n",
    "                for word in tokenizer(content.replace(' ','')):\n",
    "                    vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size]\n",
    "    vocab_list = [(PAD, -1), (UNK, -1)] + vocab_list\n",
    "    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n",
    "    return vocab_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T08:53:41.794764Z",
     "start_time": "2021-06-03T08:53:41.792533Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_id = build_vocab(train_path, tokenizer, None, 20)\n",
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "pkl.dump(word_to_id, open(vocab_dir, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T07:01:22.811886Z",
     "start_time": "2021-06-03T07:01:22.804948Z"
    }
   },
   "outputs": [],
   "source": [
    "### 如果上边build vocab已经运行过一次，则直接加载即可\n",
    "if os.path.exists(vocab_dir):\n",
    "    word_to_id = pkl.load(open(vocab_dir, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T07:01:25.401072Z",
     "start_time": "2021-06-03T07:01:25.374876Z"
    }
   },
   "source": [
    "## 1.2 根据vocab生成腾讯预训练向量并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T07:47:22.657090Z",
     "start_time": "2021-06-03T07:47:22.622106Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 200\n",
    "filename_trimmed_dir = \"./THUCNews/data/embedding_tencent_words\"\n",
    "\n",
    "embeddings = np.random.rand(len(word_to_id), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T07:47:35.205644Z",
     "start_time": "2021-06-03T07:47:35.138476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12062/12062 [00:00<00:00, 193283.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for k,v in tqdm(word_to_id.items()):\n",
    "    if k in wv_from_text:\n",
    "        embeddings[v] = np.asarray(wv_from_text[k], dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T08:23:16.258642Z",
     "start_time": "2021-06-03T08:23:12.927667Z"
    }
   },
   "outputs": [],
   "source": [
    "#保存\n",
    "np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T08:24:59.203609Z",
     "start_time": "2021-06-03T08:24:58.966437Z"
    }
   },
   "outputs": [],
   "source": [
    "#加载\n",
    "emb_tc = torch.tensor(np.load(filename_trimmed_dir + '.npz')[\"embeddings\"].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 数据处理，根据当前场景语料生成字向量和vocab_chars.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:18:43.732855Z",
     "start_time": "2021-06-09T06:15:49.714187Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "fd='~/pretrained/sogou_embedding/sgns.sogou.char.bz2'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(fd, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 根据场景语料build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:22:53.397327Z",
     "start_time": "2021-06-09T06:22:52.815803Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.txt文件格式\n",
    "'''\n",
    "体验2D巅峰 倚天屠龙记十大创新概览\t8\n",
    "60年铁树开花形状似玉米芯(组图)\t5\n",
    "同步A股首秀：港股缩量回调\t2\n",
    "中青宝sg现场抓拍 兔子舞热辣表演\t8\n",
    "锌价难续去年辉煌\t0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:23:42.773923Z",
     "start_time": "2021-06-09T06:23:42.770826Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_char = lambda x: [y for y in x]\n",
    "train_path = ['./THUCNews/data/train.txt',\n",
    "             './THUCNews/data/dev.txt',\n",
    "             './THUCNews/data/test.txt']\n",
    "vocab_dir = \"./THUCNews/data/vocab_chars.pkl\"\n",
    "UNK, PAD = '<UNK>', '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:23:43.499751Z",
     "start_time": "2021-06-09T06:23:43.493816Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab_char(files_path, tokenizer, max_size, min_freq):\n",
    "    #max_size取None则表示不指定max size大小\n",
    "    vocab_dic = {}\n",
    "    if isinstance(files_path, str):\n",
    "        raise ValueError('please input paths in list format!')\n",
    "    for file_path in files_path:\n",
    "        with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                content = lin.split('\\t')[0]\n",
    "                for word in tokenizer_char(content.replace(' ','')):\n",
    "                    vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size]\n",
    "    vocab_list = [(PAD, -1), (UNK, -1)] + vocab_list\n",
    "    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n",
    "    return vocab_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:23:59.802129Z",
     "start_time": "2021-06-09T06:23:58.623083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [00:01, 179433.89it/s]\n",
      "10000it [00:00, 146745.83it/s]\n",
      "10000it [00:00, 121749.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_id = build_vocab_char(train_path, tokenizer_char, None, 1)\n",
    "len(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:24:29.192861Z",
     "start_time": "2021-06-09T06:24:29.189121Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "pkl.dump(char_to_id, open(vocab_dir, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 如果上边build vocab已经运行过一次，则直接加载即可\n",
    "if os.path.exists(vocab_dir):\n",
    "    char_to_id = pkl.load(open(vocab_dir, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 根据vocab生成搜狗预训练字向量并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:29:58.847839Z",
     "start_time": "2021-06-09T06:29:58.824815Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "filename_trimmed_dir = \"./THUCNews/data/embedding_sogou_chars\"\n",
    "embeddings = np.random.rand(len(char_to_id), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:30:07.554660Z",
     "start_time": "2021-06-09T06:30:07.508054Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4802/4802 [00:00<00:00, 115349.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for k,v in tqdm(char_to_id.items()):\n",
    "    if k in model:\n",
    "        embeddings[v] = np.asarray(model[k], dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:30:38.951994Z",
     "start_time": "2021-06-09T06:30:36.910165Z"
    }
   },
   "outputs": [],
   "source": [
    "#保存\n",
    "np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载\n",
    "emb_sg = torch.tensor(np.load(filename_trimmed_dir + '.npz')[\"embeddings\"].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pl_torch] *",
   "language": "python",
   "name": "conda-env-pl_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}